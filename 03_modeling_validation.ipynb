{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Three: Modeling & Validation (Pure Polars)\n",
    "## Portfolio Management PPI Modeling Project\n",
    "\n",
    "**Objective**: Implement and validate forecasting models for PPI MoM% changes using pure Polars.\n",
    "\n",
    "**Model Hierarchy**:\n",
    "- Baseline AR models\n",
    "- Ridge/Lasso regression with lagged features\n",
    "- Random Forest for non-linear relationships\n",
    "- Time series cross-validation framework\n",
    "\n",
    "**Validation Framework**:\n",
    "- Time series cross-validation (expanding window)\n",
    "- Out-of-sample testing\n",
    "- Walk-forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# Import our modeling framework\nfrom modeling_framework import TimeSeriesModeler\nfrom exploratory_analysis import ExploratoryAnalyzer\n\n# Configure plotting\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"Phase Three setup complete - Modeling & Validation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize modeler\nmodeler = TimeSeriesModeler()\n\n# Load analysis data from Phase Two\nprint(\"Loading analysis data from Phase Two...\")\nanalysis_df = modeler.load_analysis_data()\n\nif analysis_df is not None:\n    print(f\"Data loaded: {analysis_df.shape}\")\n    print(f\"Date range: {analysis_df['date'].min()} to {analysis_df['date'].max()}\")\n    print(f\"Columns: {analysis_df.columns[:10]}...\")  # Show first 10 columns\nelse:\n    print(\"WARNING: Failed to load data. Please run Phase Two first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if analysis_df is not None:\n    # Create lagged features\n    target_col = 'mom_pct'\n    predictor_cols = [col for col in analysis_df.columns if col not in ['date', target_col]]\n    \n    print(f\"Creating lagged features for {len(predictor_cols)} predictors...\")\n    \n    # Limit predictors for performance (top 10)\n    top_predictors = predictor_cols[:10]\n    \n    feature_df = modeler.create_lagged_features(\n        analysis_df, target_col, top_predictors, max_lags=3\n    )\n    \n    print(f\"Enhanced dataset: {feature_df.shape}\")\n    print(f\"Features created: {len(feature_df.columns) - 2}\")  # Exclude date and target\n    \n    # Show sample of enhanced data\n    print(\"\\nSample of enhanced dataset:\")\n    print(feature_df.head())\n    \nelse:\n    print(\"Cannot create features - no data available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Splitting for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feature_df' in locals():\n",
    "    # Split data for time series validation\n",
    "    train_df, val_df, test_df = modeler.time_series_split(feature_df)\n",
    "    \n",
    "    print(\"Time Series Data Splits:\")\n",
    "    print(f\"Training:   {train_df.shape} - {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "    print(f\"Validation: {val_df.shape} - {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "    print(f\"Test:       {test_df.shape} - {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "    \n",
    "    # Visualize the splits\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot target variable for each split\n",
    "    train_dates = train_df['date'].to_numpy()\n",
    "    train_values = train_df[target_col].to_numpy()\n",
    "    \n",
    "    val_dates = val_df['date'].to_numpy()\n",
    "    val_values = val_df[target_col].to_numpy()\n",
    "    \n",
    "    test_dates = test_df['date'].to_numpy()\n",
    "    test_values = test_df[target_col].to_numpy()\n",
    "    \n",
    "    ax.plot(train_dates, train_values, 'b-', label='Training', alpha=0.7)\n",
    "    ax.plot(val_dates, val_values, 'g-', label='Validation', alpha=0.7)\n",
    "    ax.plot(test_dates, test_values, 'r-', label='Test', alpha=0.7)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Time Series Data Splits', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('PPI MoM % Change')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot split data - feature dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_df is not None:\n",
    "    # Run comprehensive model comparison\n",
    "    print(\"Running comprehensive model comparison...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    comparison_results = modeler.run_model_comparison(analysis_df, target_col='mom_pct')\n",
    "    \n",
    "    # Print detailed summary\n",
    "    modeler.print_model_summary(comparison_results)\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot run model comparison - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_results' in locals() and comparison_results['results']:\n",
    "    results = comparison_results['results']\n",
    "    \n",
    "    # Extract metrics for visualization\n",
    "    model_names = list(results.keys())\n",
    "    rmse_values = [results[name].get('rmse', 0) for name in model_names]\n",
    "    mae_values = [results[name].get('mae', 0) for name in model_names]\n",
    "    dir_acc_values = [results[name].get('directional_accuracy', 0) for name in model_names]\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[0].bar(model_names, rmse_values, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('RMSE Comparison', fontweight='bold')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[1].bar(model_names, mae_values, color='lightcoral', alpha=0.7)\n",
    "    axes[1].set_title('MAE Comparison', fontweight='bold')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Directional Accuracy comparison\n",
    "    axes[2].bar(model_names, dir_acc_values, color='lightgreen', alpha=0.7)\n",
    "    axes[2].set_title('Directional Accuracy', fontweight='bold')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary table\n",
    "    print(\"\\nDETAILED PERFORMANCE METRICS:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for name in model_names:\n",
    "        result = results[name]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  RMSE: {result.get('rmse', 0):.4f}\")\n",
    "        print(f\"  MAE:  {result.get('mae', 0):.4f}\")\n",
    "        print(f\"  Directional Accuracy: {result.get('directional_accuracy', 0):.3f}\")\n",
    "        print(f\"  Training Samples: {result.get('n_train', 0)}\")\n",
    "        print(f\"  Features Used: {len(result.get('features', []))}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No model results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_results' in locals() and comparison_results['results']:\n",
    "    # Feature importance for tree-based models\n",
    "    for model_name, result in comparison_results['results'].items():\n",
    "        if 'feature_importance' in result:\n",
    "            importance = result['feature_importance']\n",
    "            \n",
    "            # Sort by importance\n",
    "            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Plot top 15 features\n",
    "            top_features = sorted_features[:15]\n",
    "            feature_names = [item[0] for item in top_features]\n",
    "            feature_values = [item[1] for item in top_features]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(range(len(feature_names)), feature_values, color='steelblue', alpha=0.7)\n",
    "            plt.yticks(range(len(feature_names)), feature_names)\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'{model_name} - Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n{model_name} - Top 10 Most Important Features:\")\n",
    "            for i, (feature, imp) in enumerate(top_features[:10]):\n",
    "                print(f\"  {i+1:2d}. {feature:<30} {imp:.4f}\")\n",
    "            \n",
    "            break  # Only show for first tree model\n",
    "    \n",
    "else:\n",
    "    print(\"No feature importance data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Diagnostics and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"phase three: modeling & validation summary\")\nprint(\"=\" * 60)\n\nif 'comparison_results' in locals():\n    results = comparison_results['results']\n    data_info = comparison_results['data_splits']\n    \n    print(f\"\\ndata summary:\")\n    print(f\"   • training data: {data_info['train_shape']}\")\n    print(f\"   • validation data: {data_info['val_shape']}\")\n    print(f\"   • test data: {data_info['test_shape']}\")\n    print(f\"   • total features: {comparison_results['feature_count']}\")\n    \n    if results:\n        print(f\"\\nmodel performance:\")\n        \n        # Find best model by RMSE\n        best_model = min(results.items(), key=lambda x: x[1].get('rmse', float('inf')))\n        print(f\"   • best model: {best_model[0]}\")\n        print(f\"   • best rmse: {best_model[1]['rmse']:.4f}\")\n        print(f\"   • best directional accuracy: {best_model[1].get('directional_accuracy', 0):.3f}\")\n        \n        # Model comparison\n        print(f\"\\nall models:\")\n        for name, result in results.items():\n            rmse = result.get('rmse', 0)\n            dir_acc = result.get('directional_accuracy', 0)\n            print(f\"   • {name}: rmse={rmse:.4f}, dir.acc={dir_acc:.3f}\")\n        \n        # Economic interpretation\n        print(f\"\\neconomic insights:\")\n        print(f\"   • target volatility: ~{best_model[1]['rmse']:.2f}% monthly\")\n        \n        if best_model[1].get('directional_accuracy', 0) > 0.5:\n            print(f\"   • model shows predictive power (>{50:.0f}% directional accuracy)\")\n        else:\n            print(f\"   • model struggles with direction prediction\")\n        \n        print(f\"\\nphase three complete\")\n        print(f\"   • {len(results)} models successfully trained\")\n        print(f\"   • best performing model: {best_model[0]}\")\n        print(f\"   • ready for production implementation\")\n    \n    else:\n        print(\"\\nWARNING: no successful model results\")\n        \nelse:\n    print(\"\\nWARNING: no modeling results available\")\n\nprint(f\"\\nnext: phase four - real-time implementation & dashboard\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}