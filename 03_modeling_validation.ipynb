{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Three: Modeling & Validation (Pure Polars)\n",
    "## Portfolio Management PPI Modeling Project\n",
    "\n",
    "**Objective**: Implement and validate forecasting models for PPI MoM% changes using pure Polars.\n",
    "\n",
    "**Model Hierarchy**:\n",
    "- Baseline AR models\n",
    "- Ridge/Lasso regression with lagged features\n",
    "- Random Forest for non-linear relationships\n",
    "- Time series cross-validation framework\n",
    "\n",
    "**Validation Framework**:\n",
    "- Time series cross-validation (expanding window)\n",
    "- Out-of-sample testing\n",
    "- Walk-forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modeling framework\n",
    "from modeling_framework import TimeSeriesModeler\n",
    "from exploratory_analysis import ExploratoryAnalyzer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì Phase Three setup complete - Modeling & Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize modeler\n",
    "modeler = TimeSeriesModeler()\n",
    "\n",
    "# Load analysis data from Phase Two\n",
    "print(\"Loading analysis data from Phase Two...\")\n",
    "analysis_df = modeler.load_analysis_data()\n",
    "\n",
    "if analysis_df is not None:\n",
    "    print(f\"‚úì Data loaded: {analysis_df.shape}\")\n",
    "    print(f\"Date range: {analysis_df['date'].min()} to {analysis_df['date'].max()}\")\n",
    "    print(f\"Columns: {analysis_df.columns[:10]}...\")  # Show first 10 columns\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Failed to load data. Please run Phase Two first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_df is not None:\n",
    "    # Create lagged features\n",
    "    target_col = 'mom_pct'\n",
    "    predictor_cols = [col for col in analysis_df.columns if col not in ['date', target_col]]\n",
    "    \n",
    "    print(f\"Creating lagged features for {len(predictor_cols)} predictors...\")\n",
    "    \n",
    "    # Limit predictors for performance (top 10)\n",
    "    top_predictors = predictor_cols[:10]\n",
    "    \n",
    "    feature_df = modeler.create_lagged_features(\n",
    "        analysis_df, target_col, top_predictors, max_lags=3\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Enhanced dataset: {feature_df.shape}\")\n",
    "    print(f\"Features created: {len(feature_df.columns) - 2}\")  # Exclude date and target\n",
    "    \n",
    "    # Show sample of enhanced data\n",
    "    print(\"\\nSample of enhanced dataset:\")\n",
    "    print(feature_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create features - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Splitting for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feature_df' in locals():\n",
    "    # Split data for time series validation\n",
    "    train_df, val_df, test_df = modeler.time_series_split(feature_df)\n",
    "    \n",
    "    print(\"Time Series Data Splits:\")\n",
    "    print(f\"Training:   {train_df.shape} - {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "    print(f\"Validation: {val_df.shape} - {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "    print(f\"Test:       {test_df.shape} - {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "    \n",
    "    # Visualize the splits\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot target variable for each split\n",
    "    train_dates = train_df['date'].to_numpy()\n",
    "    train_values = train_df[target_col].to_numpy()\n",
    "    \n",
    "    val_dates = val_df['date'].to_numpy()\n",
    "    val_values = val_df[target_col].to_numpy()\n",
    "    \n",
    "    test_dates = test_df['date'].to_numpy()\n",
    "    test_values = test_df[target_col].to_numpy()\n",
    "    \n",
    "    ax.plot(train_dates, train_values, 'b-', label='Training', alpha=0.7)\n",
    "    ax.plot(val_dates, val_values, 'g-', label='Validation', alpha=0.7)\n",
    "    ax.plot(test_dates, test_values, 'r-', label='Test', alpha=0.7)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    ax.set_title('Time Series Data Splits', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('PPI MoM % Change')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot split data - feature dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_df is not None:\n",
    "    # Run comprehensive model comparison\n",
    "    print(\"Running comprehensive model comparison...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    comparison_results = modeler.run_model_comparison(analysis_df, target_col='mom_pct')\n",
    "    \n",
    "    # Print detailed summary\n",
    "    modeler.print_model_summary(comparison_results)\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot run model comparison - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_results' in locals() and comparison_results['results']:\n",
    "    results = comparison_results['results']\n",
    "    \n",
    "    # Extract metrics for visualization\n",
    "    model_names = list(results.keys())\n",
    "    rmse_values = [results[name].get('rmse', 0) for name in model_names]\n",
    "    mae_values = [results[name].get('mae', 0) for name in model_names]\n",
    "    dir_acc_values = [results[name].get('directional_accuracy', 0) for name in model_names]\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[0].bar(model_names, rmse_values, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('RMSE Comparison', fontweight='bold')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[1].bar(model_names, mae_values, color='lightcoral', alpha=0.7)\n",
    "    axes[1].set_title('MAE Comparison', fontweight='bold')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Directional Accuracy comparison\n",
    "    axes[2].bar(model_names, dir_acc_values, color='lightgreen', alpha=0.7)\n",
    "    axes[2].set_title('Directional Accuracy', fontweight='bold')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary table\n",
    "    print(\"\\nDETAILED PERFORMANCE METRICS:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for name in model_names:\n",
    "        result = results[name]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  RMSE: {result.get('rmse', 0):.4f}\")\n",
    "        print(f\"  MAE:  {result.get('mae', 0):.4f}\")\n",
    "        print(f\"  Directional Accuracy: {result.get('directional_accuracy', 0):.3f}\")\n",
    "        print(f\"  Training Samples: {result.get('n_train', 0)}\")\n",
    "        print(f\"  Features Used: {len(result.get('features', []))}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No model results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_results' in locals() and comparison_results['results']:\n",
    "    # Feature importance for tree-based models\n",
    "    for model_name, result in comparison_results['results'].items():\n",
    "        if 'feature_importance' in result:\n",
    "            importance = result['feature_importance']\n",
    "            \n",
    "            # Sort by importance\n",
    "            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Plot top 15 features\n",
    "            top_features = sorted_features[:15]\n",
    "            feature_names = [item[0] for item in top_features]\n",
    "            feature_values = [item[1] for item in top_features]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(range(len(feature_names)), feature_values, color='steelblue', alpha=0.7)\n",
    "            plt.yticks(range(len(feature_names)), feature_names)\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'{model_name} - Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n{model_name} - Top 10 Most Important Features:\")\n",
    "            for i, (feature, imp) in enumerate(top_features[:10]):\n",
    "                print(f\"  {i+1:2d}. {feature:<30} {imp:.4f}\")\n",
    "            \n",
    "            break  # Only show for first tree model\n",
    "    \n",
    "else:\n",
    "    print(\"No feature importance data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Diagnostics and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE THREE: MODELING & VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'comparison_results' in locals():\n",
    "    results = comparison_results['results']\n",
    "    data_info = comparison_results['data_splits']\n",
    "    \n",
    "    print(f\"\\nüìä DATA SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Training data: {data_info['train_shape']}\")\n",
    "    print(f\"   ‚Ä¢ Validation data: {data_info['val_shape']}\")\n",
    "    print(f\"   ‚Ä¢ Test data: {data_info['test_shape']}\")\n",
    "    print(f\"   ‚Ä¢ Total features: {comparison_results['feature_count']}\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüèÜ MODEL PERFORMANCE:\")\n",
    "        \n",
    "        # Find best model by RMSE\n",
    "        best_model = min(results.items(), key=lambda x: x[1].get('rmse', float('inf')))\n",
    "        print(f\"   ‚Ä¢ Best Model: {best_model[0]}\")\n",
    "        print(f\"   ‚Ä¢ Best RMSE: {best_model[1]['rmse']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Best Directional Accuracy: {best_model[1].get('directional_accuracy', 0):.3f}\")\n",
    "        \n",
    "        # Model comparison\n",
    "        print(f\"\\nüìà ALL MODELS:\")\n",
    "        for name, result in results.items():\n",
    "            rmse = result.get('rmse', 0)\n",
    "            dir_acc = result.get('directional_accuracy', 0)\n",
    "            print(f\"   ‚Ä¢ {name}: RMSE={rmse:.4f}, Dir.Acc={dir_acc:.3f}\")\n",
    "        \n",
    "        # Economic interpretation\n",
    "        print(f\"\\nüí° ECONOMIC INSIGHTS:\")\n",
    "        print(f\"   ‚Ä¢ Target volatility: ~{best_model[1]['rmse']:.2f}% monthly\")\n",
    "        \n",
    "        if best_model[1].get('directional_accuracy', 0) > 0.5:\n",
    "            print(f\"   ‚Ä¢ Model shows predictive power (>{50:.0f}% directional accuracy)\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ Model struggles with direction prediction\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ PHASE THREE COMPLETE\")\n",
    "        print(f\"   ‚Ä¢ {len(results)} models successfully trained\")\n",
    "        print(f\"   ‚Ä¢ Best performing model: {best_model[0]}\")\n",
    "        print(f\"   ‚Ä¢ Ready for production implementation\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No successful model results\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No modeling results available\")\n",
    "\n",
    "print(f\"\\nNext: Phase Four - Real-time Implementation & Dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
